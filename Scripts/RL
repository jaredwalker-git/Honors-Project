'''
Initialize Buffer in __init__ for storing each variable of length size
Class replay w attribute for state with size stocks x timesteps x features -> going to first try for one stock
Modulous index with maxsize -> sets index back to 0
'''

from matplotlib.pyplot import hist, sca
import numpy as np
from sklearn.preprocessing import StandardScaler
import itertools
import argparse
import os
import pickle
import matplotlib.pyplot as plt

from tensorflow import keras
from tensorflow.python.keras.engine.sequential import Sequential
from tensorflow.python.ops.gen_math_ops import tanh
from tensorflow.python.summary.summary import scalar

'''
Function to Standardize Data
We need to standardize data before using it -> no reward data since no actions have been performed by agent -> untrained agent so just randomly sample actions for num_steps in episode
Info is portfolio data
'''

def grab_data(name):

    raw_data = np.load(name, allow_pickle = True) 
    tickers = ['AAL', 'AAPL']
    #initialize array for data at # tickers x timesteps x features -> features are explicitly set since no good way to generalize this aspect for any data
    input_data = np.zeros(shape = (len(tickers), raw_data[1].shape[0], 5))
    #j is index for new data, and will be incremented from 0 to num_stocks as tickers are found in raw data
    j = 0

    for i in range(len(raw_data)):
        if raw_data[i][0, 6] in tickers:
            input_data[j, :, :] = raw_data[i][:, 1:6]
            j = j + 1
    return input_data

def make_portfolio_hist():
    plt.hist(portfolio_value, bins = len(portfolio_value)//2)
    plt.xlabel('Value')
    plt.ylabel('Frequency')
    plt.show()
    plt.plot(np.arange(len(portfolio_value)), portfolio_value)
    plt.show()

def makedir(name):
    if os.path.exists(name):
        return
    os.makedirs(name)

class Sample_Memory:
    #setting attributes for state and next state (stock data), action, reward, done flag, index, current size and max size of buffer
    def __init__(self, state_size, action_size, buf_size):
        self.state = np.empty(shape = (buf_size, state_size)) 
        self.state1 = np.empty(shape = (buf_size, state_size))
        self.action = np.empty (buf_size)
        self.reward = np.empty(buf_size)
        self.done_flag = np.empty(buf_size)
        self.index = 0 
        self.curr_size = 0
        self.maxsize = buf_size
    #function to store obs of episode to buffer and increment index for next entry to buffer until maxsize
    def store_data(self, curr_state, next_state, act, rew, done):
        self.state[self.index] = curr_state
        self.state1[self.index] = next_state
        self.action[self.index] = act
        self.reward[self.index] = rew
        self.done_flag[self.index] = done
        self.index = (self.index + 1) % self.maxsize
        size = self.curr_size+1
        self.curr_size = size
        #setting the size to the minimum btwn curr_size and max size so once the buffer is full it will not grow any more
        self.curr_size = min(self.curr_size, self.maxsize)
    #function to sample batch size from buffer up to index at curr_size of buffer
    def sample(self, batchsize):
        indices = np.random.randint(0, self.curr_size, size = batchsize)
        return dict(states = self.state[indices], states1 = self.state1[indices], actions = self.action[indices], rewards = self.reward[indices], done = self.done_flag[indices])

class StockEnv:
    #initializing attributes
    def __init__(self, data, initial_investment):
        #data is num_stocks, timesteps, features
        self.stock_data = data
        self.num_stocks = self.stock_data.shape[0]
        self.num_steps = self.stock_data.shape[1]
        self.num_features = self.stock_data.shape[2]
        self.initial_investment = initial_investment
        self.step_index = 0
        self.stock_owned = np.zeros(self.num_stocks)
        self.stock_price = np.zeros(self.num_stocks)
        self.cash = 0
        
       
        #number of different action = 3 actions ^ num_stocks -   
        self.action_list = np.arange( 3 ** self.num_stocks )
        #enumerate all action space by creating cartesian products of array repeated number of times = number of stocks to act on
        self.action_space = list(itertools.product([0 , 1, 2], repeat = self.num_stocks))
        #state space for price, quantity, num features for each stock +1 for cash held
        self.state_size = 2 * self.num_stocks + self.num_features * self.num_stocks + 1

    def get_stock_price(self):
        self.stock_price = np.zeros(shape = self.num_stocks)
        for j in range(self.num_stocks):
            #set each stock price to that at initial step
                      
            #print(f'STOCK DATA INDEX FEATURE 0: {self.stock_data[j, self.step_index, 0]}')
            self.stock_price[j] = self.stock_data[j, self.step_index, 0]
            
    def reset(self):
        self.stock_owned = np.zeros(self.num_stocks)
        self.stock_price = np.zeros(self.num_stocks)
        self.step_index = 0
        self.get_stock_price() 
        self.cash = self.initial_investment
        
        return self.get_state()
    #this is where policy evaluation happens, as an action is passed in an the resulting states and rewards are calculated and returned
    def step(self, action):
        #value of portfolio before step
        pre_val  = self.get_val()
        self.step_index += 1
        #get stock prices for this index
        self.get_stock_price()
        self.trade(action)
        post_val = self.get_val()
        reward = post_val - pre_val
        #set done flag to boolean expression checking index against num_steps
        done = (self.step_index == self.num_steps - 1)    
        info = {'val': post_val}
        return self.get_state(), reward, done, info

    def get_state(self):
        #initialize state
        state = np.empty(shape = self.state_size)
        #set first indices to quantities of each stock
        state[:self.num_stocks] = self.stock_owned
        #set next indices to price of each stock
        state[self.num_stocks:2*self.num_stocks] = self.stock_price
        #set final indices to features of each stock
        for k in range(self.num_stocks):
            #indexing num_features spaces at a time for num_stocks so that features for each stock are saved in state space
            state[2*self.num_stocks+(k*num_features):2*self.num_stocks+(k*num_features)+self.num_features] = self.stock_data[k, self.step_index, :]
        return state
    
    def get_val(self):
        val = self.stock_owned.dot(self.stock_price)
        return val

    def trade(self, action):
        action_vec = self.action_space[action]
        buy_stocks = []
        sell_stocks = []
        #0 buy, 1 hold, 2 sell
        for i, a in enumerate(action_vec):
            if a == 2:
                sell_stocks.append(i)
            elif a == 0:
                buy_stocks.append(i)

        if sell_stocks:
            for i in sell_stocks:
                self.cash += self.stock_price[i] * self.stock_owned[i] 
                self.stock_owned[i] = 0

        if buy_stocks:
            canbuy = True
            while canbuy:
                for i in buy_stocks:
                    if self.cash > self.stock_price[i]:
                        self.cash -= self.stock_price[i]
                        self.stock_owned[i] += 1   
                    else:
                        canbuy = False
         
    def standardize_data(self):
        #need to get scalar for enviroment,first need to step through for num_steps to populate num_stocks and cash info
        states = [] #set list to fill with states given random action
        for _ in range(self.num_steps - 1):
            action = np.random.choice(self.action_list)
            state, reward, done, info = self.step(action)
            states.append(state)
            if done:
                break
    
        standard = StandardScaler()
        standard.fit(states)
        return standard   
 
class Agent:
    def __init__(self, stateShape, actionSize):
        self.state_shape = stateShape
        self.action_size = actionSize
        self.memory = Sample_Memory(stateShape, actionSize, 500)
        self.gamma = 0.9 #gamma is scaling factor for future return in value function
        self.epsilon = 1.0 #starts at 1 which is completely random action
        self.epsilon_min = 0.01 #always have 1% chance of random action -> let agent explore
        self.epsilon_decay = 0.995 #as the agent learns, want to act less randomly and more on experience
        self.model = Sequential()
        self.make_model(stateShape, actionSize, 32)
    
    def update_memory(self, state, state1, act, rew, done):
        self.memory.store_data(state, state1, act, rew, done)
    #this is where policy improvement happens, a* = argmax(Q(s,a))
    def get_act(self, state):
        #this is excersizing of a policy with exploration -> Policy improvement 
        if  np.random.rand() <= self.epsilon:
            return (np.random.choice(self.action_size))
        act_val = self.model.predict(state) #action-value function given policy
        return np.argmax(act_val[0]) #return optimal action
    
    def learn(self, batchsize):
        if batchsize > self.memory.curr_size:
            return
        batch = self.memory.sample(batchsize)
        states = batch['states']
        states1 = batch['states1']
        rewards = batch['rewards']
        #target return estimated -> this is the prediction problem of reinforcement learning
        #this is using value iteration to train the best policy -> deep Q-Learning
        targets = rewards + self.gamma * np.amax(self.model.predict(states1), axis = 1)
        #train on batch -> this is the control problem of reinforcement learning -> finding an optimal policy based on the expected value of the future return
        self.model.train_on_batch(states, targets)

        #check epsilon and have it decay so that as the model learns here it relies less on exploration
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
            
    def make_model(self, input_shape, num_actions, nodes):
        # making model 
        self.model = keras.models.Sequential()
        self.model.add(keras.layers.Input(shape = (input_shape)))
        self.model.add(keras.layers.Dense(nodes, activation = 'relu')) 
        self.model.add(keras.layers.Dense(nodes, activation = 'relu')) 
        self.model.add(keras.layers.Dense(nodes, activation = 'relu')) 
        self.model.add(keras.layers.Dense(num_actions)) 
        self.model.compile(loss = 'mse', optimizer = keras.optimizers.Adam(lr = 0.001))
    
def play_episode(agent, env, mode):
    #reset env and standardize state data
    state = env.reset()
    state = standard.transform([state])
    done = False

    while not done:
        #get action given state using current policy
        act = agent.get_act(state)
        #evaluation of a policy
        state1, rew, done, info = env.step(act)
        #standardize next state
        state1 = standard.transform([state1])
        #improvement of the policy
        if (mode == 'train') or (mode == 'train_cont'):
            agent.update_memory(state, state1, act, rew, done) 
            agent.learn(batchSize)
        state = state1
        
    return info['val']

'''Main'''

if __name__ == '__main__':
    #config for preset parameters
    reward_folder = 'rl_rewards'
    model_folder = 'rl_models'
    num_episodes = 200
    investment_val = 100000
    batchSize = 32


    #set up parser for command line accessibility
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', type = str, required = True, help = 'choose either "train", "train_cont", "list" or "test"')
    args = parser.parse_args()
    
    #make dir for folders
    makedir(reward_folder)
    makedir(model_folder)

    #grab data and pull dimensions from it
    data = grab_data('inputData.npy')
    num_stocks = data.shape[0]
    num_timesteps = data.shape[1]
    num_features = data.shape[2]
       
    #split to test vs train data
    '''
    t = 10 #number of time stamps for each sequence                         <THIS MAY BE NECESSARY
    n = len(data) - t #length of usable timesteps for series creation
    '''
    #choosing amount of data for training
    num_train = num_timesteps // 2
    #Initialize Time Series inputs
    train_data = np.zeros(shape = (num_stocks, num_train, num_features))
    test_data = np.zeros(shape = (num_stocks, num_train + 1, num_features))
    
    for n in range(num_stocks):
        train_data[n, :, :] = data[n, :num_train, :]
        test_data[n, :, :] = data[n, num_train:, :]

    if args.mode == 'list':
        raw_data = np.load('inputData.npy', allow_pickle = True)
        print('Tickers')
        for i in range(len(raw_data)):
            print(raw_data[i][0, 6])
        
        exit()
        
        


    enviroment = StockEnv(train_data, investment_val)
    state_size = enviroment.state_size
    num_actions = len(enviroment.action_list)
    agent = Agent(state_size, num_actions)
    standard = enviroment.standardize_data()
    portfolio_value = []

    #if mode is test, model has already been trained, so load and reset enviroment below -> also must load existing weights
    if args.mode == 'test':
        with open(f'{model_folder}/standard.pkl', 'rb') as file: #using with statement and formatted literal strings to open weight file
            standard = pickle.load(file)
        enviroment = StockEnv(test_data, investment_val)
        agent.epsilon = 0.01 #lower epsilon now that agent should be pretty good at choosing action given state, keep epsilon above 0 so that trader is not deterministic
        agent.model.load_weights(f'{model_folder}/policy.h5')
        print('Model Load Complete')
    elif args.mode == 'train_cont':
        with open(f'{model_folder}/standard.pkl', 'rb') as file: #using with statement and formatted literal strings to open weight file
            standard = pickle.load(file)
        enviroment = StockEnv(test_data, investment_val)
        agent.model.load_weights(f'{model_folder}/policy.h5')
        print('Model Load Complete')

    
    for episode in range(num_episodes):
        print(f'Episode {episode + 1} running...')
        value = play_episode(agent, enviroment, args.mode)
        portfolio_value.append(value)
        print(f'Portfolio Value: {value} ')
        
    
    #if mode is train, need to save resulting weights and enviroment created from playing all episodes
    if (args.mode == 'train') or (args.mode == 'train_cont'):
        agent.model.save_weights(f'{model_folder}/policy4stocks.h5')
        with open(f'{model_folder}/standard.pkl', 'wb') as f: #must specify write and binary to avoid errors in loading
            pickle.dump(standard, f)
 
    make_portfolio_hist()
    np.save(f'{reward_folder}/{args.mode}.npy', portfolio_value)

    



    
    


    
