'''
Initialize Buffer in __init__ for storing each variable of length size
Class replay w attribute for state with size stocks x timesteps x features -> going to first try for one stock
Modulous index with maxsize -> sets index back to 0
'''

from matplotlib.pyplot import sca
import numpy as np
from sklearn.preprocessing import StandardScaler
import itertools
import argparse
import os
import pickle
import tensorflow
from tensorflow import keras
from tensorflow.python.keras.engine.sequential import Sequential
from tensorflow.python.ops.gen_math_ops import tanh
from tensorflow.python.summary.summary import scalar

'''
Function to Standardize Data
We need to standardize data before using it -> no reward data since no actions have been performed by agent -> untrained agent so just randomly sample actions for num_steps in episode
Info is portfolio data
'''

def grab_data(name):

    raw_data = np.load(name, allow_pickle = True) 
    tickers = ['AAL', 'AAPL']
    #initialize array for data at # tickers x timesteps x features -> features are explicitly set since no good way to generalize this aspect for any data
    input_data = np.zeros(shape = (len(tickers), raw_data[1].shape[0], 5))
    #j is index for new data, and will be incremented from 0 to num_stocks as tickers are found in raw data
    j = 0

    for i in range(len(raw_data)):
        if raw_data[i][0, 6] in tickers:
            input_data[j, :, :] = raw_data[i][:, 1:6]
            j = j + 1
    return input_data

def makedir(name):
    if os.path.exists:
        return
    os.makedirs(name)

class Replay_Memory:
    #setting attributes for state and next state (stock data), action, reward, done flag, index, current size and max size of buffer
    def __init__(self, state_size, action_size, buf_size):
        self.state = np.empty(shape = (buf_size, state_size)) 
        self.state1 = np.empty(shape = (buf_size, state_size))
        self.action = np.empty (buf_size)
        self.reward = np.empty(buf_size)
        self.done_flag = np.empty(buf_size)
        self.index = 0 
        self.curr_size = 0
        self.maxsize = buf_size
    #function to store obs of episode to buffer and increment index for next entry to buffer until maxsize
    def store_data(self, curr_state, next_state, act, rew, done):
        self.state[self.index] = curr_state
        self.state1[self.index] = next_state
        self.action[self.index] = act
        self.reward[self.index] = rew
        self.done_flag[self.index] = done
        self.index = (self.index + 1) % self.maxsize
    #function to sample batch size from buffer up to index at curr_size of buffer
    def sample(self, batchsize):
        indices = np.random.randint(0, self.curr_size, size = batchsize)
        return (self.state[indices], self.state1[indices], self.action[indices], self.reward[indices], self.done_flag[indices])

class StockEnv:
    #initializing attributes
    def __init__(self, data, initial_investment):
        #data is num_stocks, timesteps, features
        self.stock_data = data
        self.num_stocks = self.stock_data.shape[0]
        self.num_steps = self.stock_data.shape[1]
        self.num_features = self.stock_data.shape[2]
        self.initial_investment = initial_investment
        self.step_index = None
        self.stock_owned = None
        self.stock_price = None
        self.cash = None
        
       
        #number of different action = 3 actions ^ num_stocks -   
        self.action_list = np.arange( 3 ** self.num_stocks )
        #enumerate all action space by creating cartesian products of array repeated number of times = number of stocks to act on
        self.action_space = list(itertools.product([0 , 1, 2], repeat = self.num_stocks) )
        #state space for price, quantity, num features for each stock +1 for cash held
        self.state_size = 2 * self.num_stocks + self.num_features * self.num_stocks + 1

    def get_stock_price(self):
        for j in range(self.num_stocks):
            #set each stock price to that at initial step
            self.stock_price[j] = self.stock_data[j, 0, 0]

    def reset(self):
        self.stock_owned = np.zeros(self.num_stocks)
        self.stock_price = np.zeros(self.num_stocks)
        self.step_index = 0
        self.get_stock_price() 
        self.cash = self.initial_investment
        
        return self.get_state()
    
    #this is where policy evaluation happens, as an action is passed in an the resulting states and rewards are calculated and returned
    def step(self, action):
        #value of portfolio before step
        pre_val  = self.get_val()
        self.step_index += 1
        #get stock prices for this index
        self.get_stock_price()
        self.trade(action)
        post_val = self.get_val()
        reward = post_val - pre_val
        #set done flag to boolean expression checking index against num_steps
        done = (self.step_index == self.num_steps - 1) #set done flag to true once the step index has reached the num of steps
        info = {'Val:', post_val}
        return self.get_state(), reward, done, info

    def get_state(self):
        #initialize state
        state = np.empty(shape = self.state_size)
        #set first indices to quantities of each stock
        state[:self.num_stocks] = self.stock_owned
        #set next indices to price of each stock
        state[self.num_stocks:2*self.num_stocks] = self.stock_price
        #set final indices to features of each stock
        for k in range(self.num_stocks):
            #indexing num_features spaces at a time for num_stocks so that features for each stock are saved in state space
            state[(k+1)*2*self.num_stocks:(k+1)*2*self.num_stocks+self.num_features] = self.stock_data[k, self.step_index, :]
        return state
    
    def get_val(self):
        val = self.stock_owned.dot(self.stock_price)
        return val

    def trade(self, action):
        action_vec = self.action_space[action]
        buy_stocks = []
        sell_stocks = []
        #0 buy, 1 hold, 2 sell
        for i, a in enumerate(action_vec):
            if a == 2:
                sell_stocks.append(i)
            elif a == 0:
                buy_stocks.append(i)

        if sell_stocks:
            for i in sell_stocks:
                self.cash += self.stock_price[i] * self.stock_owned[i] 
                self.stock_owned[i] = 0

        if buy_stocks:
            canbuy = True
            while canbuy:
                for i in buy_stocks:
                    if self.cash > self.stock_price[i]:
                        self.cash -= self.stock_price[i]
                        self.stock_owned[i] += 1   
                    else:
                        canbuy = False
                    print(f'Cash: {self.cash}')
                    print(f'OWNED: {self.stock_owned[i]}')
            
    def standardize_data(self):
        #need to get scalar for enviroment,first need to step through for num_steps to populate num_stocks and cash info
        states = [] #set list to fill with states given random action
        for _ in range(self.num_steps - 1):
            action = np.random.choice(self.action_list)
            state, reward, done, info = self.step(action)
            states.append(state)
            if done:
                break
    
        standard = StandardScaler()
        standard.fit(states)
        return standard   
 
class Agent:
    def __init__(self, stateShape, actionSize):
        self.state_shape = stateShape
        self.action_size = actionSize
        self.memory = Replay_Memory(stateShape, actionSize, 500)
        self.gamma = 0.95
        self.epsilon = 0.15
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.01
        self.model = Sequential()
        self.make_model(stateShape, actionSize, 32)
    
    def update_memory(self, state, state1, act, rew, done):
        self.memory.store_data(state, state1, act, rew, done)
    #this is where policy improvement happens, a* = argmax(Q(s,a))
    def get_act(self, state):
        #this acts as learning rate since best action will always be the same given state without exploration
        if np.random.rand(1) < self.epsilon:
            return (np.random.choice(self.action_size))
        act_val = self.model.predict(state)
        return np.argmax(act_val)
    
    def replay(self, batchsize):
        if batchsize > self.memory.curr_size:
            return
        states, states1, actions, rewards, done_flags = self.memory.sample(batchsize)
        #these predictions are the targets -> the recursivity allows for the model to make its own targets since the reward is somewhat of an indicator
        targets = rewards + self.gamma * np.amax(self.model.predict(states1), axis = 1)
        complete_target = self.model.predict(states)
        complete_target[np.arange(batchsize), actions] = targets

        #train one episode
        self.model.train_on_batch(states, complete_target)

        #check epsilon and have it decay so that as the model learns here it relies less on exploration
        if self.epsilon > self.epsilon_min:
            self.epsilon -= self.epsilon_decay

    def make_model(self, input_shape, num_actions, nodes):
        # making model 
        self.model = keras.models.Sequential()
        self.model.add(keras.layers.Input(shape = (1, input_shape)))
        #self.model.add(keras.layers.LSTM(nodes, activation = 'tanh', recurrent_activation = 'sigmoid', return_sequences = True))
        #self.model.add(keras.layers.LSTM(nodes, activation = 'tanh', recurrent_activation = 'sigmoid'))
        self.model.add(keras.layers.Dense(nodes, activation = 'tanh')) 
        self.model.add(keras.layers.Dense(num_actions)) 
        self.model.compile(loss = 'mse', optimizer= 'adam')
    
def play_episode(agent, env, mode):
    #reset env and standardize state data
    state = env.reset()
    state = env.standardize_data().transform([state])
    done = False

    while not done:
        #get action given state
        act = agent.get_act(state)
        #get next state, rew, done flag and info given action
        print(f'STEP INDEX: {env.step_index}')
        state1, rew, done, info = env.step(act)
        #standardize next state
        state1 = env.standardize_data().transform([state1])
        #test train mode
        if mode == 'train':
            agent.update_memory(state, state1, act, rew, done) 
            agent.replay(batchSize)
        state = state1
    
    print('BROKEN OUT')
    
    return info['val']

'''Main'''

if __name__ == '__main__':
    #config for preset parameters
    reward_folder = 'rl_rewards'
    model_folder = 'rl_models'
    num_episodes = 1
    investment_val = 1000
    batchSize = 32

    #set up parser for command line accessibility
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', type = str, required = True, help = 'choose either "train" or "test"')
    args = parser.parse_args()
    
    #make dir for folders
    makedir(reward_folder)
    makedir(model_folder)

    #grab data and pull dimensions from it
    data = grab_data('inputData.npy')
    num_stocks = data.shape[0]
    num_timesteps = data.shape[1]
    num_features = data.shape[2]
       
    #split to test vs train data
    '''
    t = 10 #number of time stamps for each sequence                         <THIS MAY BE NECESSARY
    n = len(data) - t #length of usable timesteps for series creation
    '''
    #choosing amount of data for training
    num_train = num_timesteps // 2
    #Initialize Time Series inputs
    train_data = np.zeros(shape = (num_stocks, num_train, num_features))
    test_data = np.zeros(shape = (num_stocks, num_train + 1, num_features))
    
    for n in range(num_stocks):
        train_data[n, :, :] = data[n, :num_train, :]
        test_data[n, :, :] = data[n, num_train:, :]

    print(f'train data time steps: {train_data.shape[1]}')
    enviroment = StockEnv(train_data, investment_val)
    print(f'num steps : {enviroment.num_steps}')
    state_size = enviroment.state_size
    num_actions = len(enviroment.action_list)
    agent = Agent(state_size, num_actions)
    #standard = standardize_data(enviroment)
    portfolio_value = []

    #if mode is test, model has already been trained, so load and reset enviroment below -> also must load existing weights
    if args.mode == 'test':
        with open(f'{model_folder}/standard.pkl') as file: #using with statement and formatted literal strings to open weight file
            standard = pickle.load(file)
        enviroment = StockEnv(test_data, investment_val)
        epsilon = 0.01 #lower epsilon now that agent should be pretty good at choosing action given state, keep epsilon above 0 so that trader is not deterministic
        agent.model.load_weights(f'{model_folder}/policy.h5')

    for episode in range(num_episodes):
        portfolio_value.append(play_episode(agent, enviroment, args.mode))
    
    #if mode is train, need to save resulting weights and enviroment created from playing all episodes
    if args.mode == 'train':
        agent.model.save_weights(f'{model_folder}/policy.h5')
        with open(f'{model_folder}/standard.pkl', 'wb') as f: #must specify write and binary to avoid errors in loading
            pickle.dump(standard, f)
    
    np.save(f'{reward_folder}/{args.mode}.npy', portfolio_value)



    
    


    
